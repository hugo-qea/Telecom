{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kaht-FPA1Jvq"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Lab2: Train a Convolutional Neural Network (CNN).\n",
    "\n",
    "In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UvxtTYHlVfRK"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HYCvhGxKWyN7"
   },
   "source": [
    "### Define LeNet\n",
    "\n",
    "![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n",
    "\n",
    "Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
    "\n",
    "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dMC_LDYdWkI7"
   },
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LeNet, self).__init__()\n",
    "    \n",
    "    # input channel = 1, output channels = 6, kernel size = 5\n",
    "    # input image size = (32, 32), image output size = (14, 14)\n",
    "    # TODO\n",
    "    self.layer1 = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\t)\n",
    "    \n",
    "    # input channel = 6, output channels = 6, kernel size = 5\n",
    "    # input image size = (14, 14), image output size = (5,5)\n",
    "    # TODO\n",
    "    self.layer2 = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\t)\n",
    "    # input dim = 400, output dim = 120\n",
    "    # TODO\n",
    "    self.fc1 = torch.nn.Linear(400, 120)\n",
    "    # input dim = 120, output dim = 84\n",
    "    # TODO\n",
    "    self.fc2 = torch.nn.Linear(120, 84)\n",
    "    # input dim = 84, output dim = 10\n",
    "    # TODO\n",
    "    self.fc3 = torch.nn.Linear(84, 10)\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # TODO\n",
    "    # Max Pooling with kernel size = 5\n",
    "    # output size = (14,14)\n",
    "    # TODO\n",
    "    x = self.layer1(x)\n",
    "    # TODO\n",
    "    # Max Pooling with kernel size = 5\n",
    "    # output size = (5, 5)\n",
    "    # TODO\n",
    "    x = self.layer2(x)\n",
    "    # flatten the feature maps into a long vector\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    \n",
    "    # TODO\n",
    "    x= F.relu(self.fc1(x))\n",
    "    # TODO\n",
    "    x= self.fc2(x)\n",
    "    # TODO\n",
    "    x= self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gChf6TvWonrV"
   },
   "source": [
    "### Define cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6j5UrBH3oek8"
   },
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "  cost_function = torch.nn.CrossEntropyLoss()\n",
    "  return cost_function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U2TjXeVdorV9"
   },
   "source": [
    "### Define the optimizer\n",
    "\n",
    "We will use SGD with learning rate-lr, weight_decay=wd and  momentum=momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hBZN-WPboulR"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(net, lr, wd, momentum):\n",
    "  optimizer =  torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "  return optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wTkfrV64oxIL"
   },
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t-sE5vFio0lf"
   },
   "outputs": [],
   "source": [
    "def test(net, data_loader, cost_function, device='cuda:0'):\n",
    "  samples = 0.\n",
    "  cumulative_loss = 0.\n",
    "  cumulative_accuracy = 0.\n",
    "\n",
    "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "      # Load data into GPU\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "        \n",
    "      # Forward pass\n",
    "      outputs = net(inputs)\n",
    "\n",
    "      # Apply the loss\n",
    "      loss = cost_function(outputs, targets)\n",
    "\n",
    "      # Better print something\n",
    "      samples+=inputs.shape[0]\n",
    "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
    "      _, predicted = outputs.max(1)\n",
    "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
    "\n",
    "\n",
    "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
    "  samples = 0.\n",
    "  cumulative_loss = 0.\n",
    "  cumulative_accuracy = 0.\n",
    "\n",
    "  \n",
    "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
    "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "    # Load data into GPU\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "      \n",
    "    # Forward pass\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # Apply the loss\n",
    "    loss = cost_function(outputs,targets)\n",
    "\n",
    "    # Reset the optimizer\n",
    "      \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Better print something, no?\n",
    "    samples+=inputs.shape[0]\n",
    "    cumulative_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "T6IT0Lsgo8AM"
   },
   "source": [
    "### Define the function that fetches a data loader that is then used during iterative training.\n",
    "\n",
    "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
    "\n",
    "***Why Normalization is needed***? \n",
    "\n",
    "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\]. \n",
    "\n",
    "***How it can be done***? \n",
    "\n",
    "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qDxpo6uVo_8k"
   },
   "outputs": [],
   "source": [
    "def get_data(batch_size, test_batch_size=256):\n",
    "  \n",
    "  # Prepare data transformations and then combine them sequentially\n",
    "  transform = list()\n",
    "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
    "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
    "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
    "\n",
    "  # Load data\n",
    "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True) \n",
    "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
    "  \n",
    "\n",
    "  # Create train and validation splits\n",
    "  num_samples = len(full_training_data)\n",
    "  training_samples = int(num_samples*0.5+1)\n",
    "  validation_samples = num_samples - training_samples\n",
    "\n",
    "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "\n",
    "  # Initialize dataloaders\n",
    "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
    "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
    "  \n",
    "  return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OHcB8f0AsY4n"
   },
   "source": [
    "### Wrapping everything up\n",
    "\n",
    "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ip_R-hruse0Q"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input arguments\n",
    "  batch_size: Size of a mini-batch\n",
    "  device: GPU where you want to train your network\n",
    "  weight_decay: Weight decay co-efficient for regularization of weights\n",
    "  momentum: Momentum for SGD optimizer\n",
    "  epochs: Number of epochs for training the network\n",
    "'''\n",
    "\n",
    "def main(batch_size=128, \n",
    "         device='cuda:0', \n",
    "         learning_rate=0.01, \n",
    "         weight_decay=0.000001, \n",
    "         momentum=0.9, \n",
    "         epochs=50):\n",
    "  \n",
    "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
    "  \n",
    "  # TODO for defining LeNet-5\n",
    "  net = LeNet()\n",
    "  \n",
    "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "  \n",
    "  cost_function = get_cost_function()\n",
    "\n",
    "  print('Before training:')\n",
    "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
    "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
    "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
    "\n",
    "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  for e in range(epochs):\n",
    "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
    "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
    "    print('Epoch: {:d}'.format(e+1))\n",
    "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "  print('After training:')\n",
    "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
    "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
    "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
    "\n",
    "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "  return net, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ltdCMiB3t18h"
   },
   "source": [
    "Lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d-z20H4tziL"
   },
   "outputs": [],
   "source": [
    "net, test_loader = main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NQBDT48CKMVC"
   },
   "source": [
    "Using the proper metric from sklearn, check which character is most frequently confused with which: can you explain why ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sm5b4MV1KzQ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "device='cuda:0'\n",
    "\n",
    "# iterate over test data\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    # Load data into GPU\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "      \n",
    "    # Forward pass\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = outputs.max(1)\n",
    "    y_pred.extend(predicted.cpu()) # Save Prediction\n",
    "    y_true.extend(targets.cpu()) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('0', '1', '2', '3', '4', '5','6','7','8','9')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix_full = confusion_matrix(y_true, y_pred)\n",
    "cf_matrix = confusion_matrix(y_true,y_pred)\n",
    "np.fill_diagonal(cf_matrix, 0)\n",
    "df_cm_ = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "df_cm_full = pd.DataFrame(cf_matrix_full / np.sum(cf_matrix_full, axis=1)[:, None], index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm_full, annot=True)\n",
    "# Assuming you have a matrix called `matrix`\n",
    "max_indices = np.unravel_index(np.argmax(cf_matrix), cf_matrix.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"The most confused classes are: \", classes[max_indices[0]], \"and\", classes[max_indices[1]])\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOvixkfeMHrD"
   },
   "source": [
    "The LeNet5 architecture can also be implemented using the sequential API ([see documentation ](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)). Reimplement it with this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    torch.flatten()\n",
    "    torch.nn.Linear(400, 120)\n",
    "    torch.nn.ReLU()\n",
    "    torch.nn.Linear(120,84)\n",
    "    torch.nn.ReLU()\n",
    "    torch.nn.Linear(84,10)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7i3-pC5xAyu5"
   },
   "source": [
    "##Experiments\n",
    "\n",
    "\n",
    "* Implement adaptive early stopping: if the validation loss did not decrease for K consecutive epochs, stop training.\n",
    "* Change dataset in order to evaluate the LeNet5 network on cifar10 dataset. You can have a look at the pytorch documentation to easily access the cifar10 dataset. \n",
    "* Try to improve performance with:\n",
    "   *   data-augmentation\n",
    "   *   dropout\n",
    "* Implement the resnet18 architecture using the Resnet18 class from pytorch.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
